{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:24px;\">Introduction to Apache Spark and Lab Overview<span>\n",
    "\n",
    "<span style=\"font-size:16px;\">\n",
    "Apache Spark is an open-source, distributed computing system designed for processing large-scale data quickly and efficiently. It provides an intuitive programming model for working with structured and unstructured data, enabling users to perform transformations and actions on datasets using high-level APIs. Spark's key features include:\n",
    "<span>\n",
    "\n",
    "#\n",
    "- <span style=\"font-size:16px;\">Speed: Spark processes data in memory, making it significantly faster than traditional disk-based frameworks like Hadoop MapReduce.<span>\n",
    "- <span style=\"font-size:16px;\">Scalability: It can scale seamlessly from a single machine to thousands of nodes in a cluster.<span>\n",
    "- <span style=\"font-size:16px;\">Versatility: Spark supports multiple workloads, including batch processing, interactive querying, streaming, machine learning, and graph processing.<span>\n",
    "\n",
    "<span style=\"font-size:16px;\">\n",
    "At the core of Spark lies the Resilient Distributed Dataset (RDD), a fault-tolerant and immutable data abstraction that enables distributed data processing. Transformations on RDDs are executed lazily, building a lineage graph that allows efficient fault recovery.\n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:24px;\">About the Lab<span>\n",
    "\n",
    "<span style=\"font-size:16px;\">\n",
    "This lab introduces key concepts and applications of Apache Spark using PySpark, Spark's Python API.<span>\n",
    "\n",
    "#\n",
    "1. <span style=\"font-size:16px;\">Basic Operations: <span>\n",
    "    - <span style=\"font-size:16px;\">Working with RDDs: Creation, transformations, and actions.<span>\n",
    "    - <span style=\"font-size:16px;\">Common transformations like map, flatMap, and groupByKey.<span>\n",
    "    - <span style=\"font-size:16px;\">Joining and aggregating datasets.<span>\n",
    "2. <span style=\"font-size:16px;\">Spark optimisations:<span>\n",
    "    - <span style=\"font-size:16px;\">Lineage tracking<span>\n",
    "    - <span style=\"font-size:16px;\">Lazy evaluation.<span>\n",
    "    - <span style=\"font-size:16px;\">Caching<span>\n",
    "    - <span style=\"font-size:16px;\">Checkpointing<span>\n",
    "3. <span style=\"font-size:16px;\">Spark UI<span>\n",
    "4. <span style=\"font-size:16px;\">Data Analytics with spark<span>\n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:24px;\">Installing pyspark<span>\n",
    "\n",
    "<span style=\"font-size:16px;\"> Ensure that you have python>=3.8 and JDK>=8.<span>\n",
    "\n",
    "<span style=\"font-size:16px;\">We can start by first installing executing the following command in your terminal/command prompt:<span>\n",
    "\n",
    "<span style=\"font-size:20px;\">pip install pyspark<span>\n",
    "\n",
    "<span style=\"font-size:16px;\">The same can be achieved by executing the following codeblock:<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\">We can verify that the installation by running the given codeblock.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\">It is also recommended to install \"findspark\" since we are using a Jupyter Notebook.\n",
    "This can be done by running the following command in the terminal.<span>\n",
    "\n",
    "<span style=\"font-size:20px;\">pip install findspark<span>\n",
    "\n",
    "<span style=\"font-size:16px;\">This can also be done by executing the given codeblock.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\">The next step is to import and initialize findspark.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\"> Now that the setup is complete, we can proceed to initialize a Spark context. The Spark context serves as the entry point to interact with Sparkâ€™s core functionalities, allowing us to create and manipulate RDDs, perform transformations, and execute actions across a cluster. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 09:26:06 WARN Utils: Your hostname, Anshiks-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.194.12.61 instead (on interface en0)\n",
      "24/12/03 09:26:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 09:26:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark\n",
    "sc = SparkContext(\"local\",\"SparkLab\")\n",
    "sc.setCheckpointDir('checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\"> When initializing the Spark context for the first time, you may encounter some warnings displaying the address and port number. Make sure to note these details, as they will be useful later for accessing SparkUI. By default, the address is set to 'localhost' and port to 4000.\n",
    "Since pyspark only allows to run one context at a time, you need to stop an older context before creating a new one. This can be done by the stop() function. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont run this cell now\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\">Once your Spark context is set up, you can begin creating Resilient Distributed Datasets (RDDs). Pyspark has multiple methods to convert different datatypes into rdd.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a Python list\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd_list = sc.parallelize(data)\n",
    "\n",
    "# Verify RDD content\n",
    "print(rdd_list.collect())  # Output: [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, World!', 'This is a sample file']\n"
     ]
    }
   ],
   "source": [
    "# Path to the file\n",
    "file_path = \"sample.txt\"\n",
    "\n",
    "# Create a sample file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"Hello, World!\\n\")\n",
    "    file.write(\"This is a sample file\")\n",
    "\n",
    "# Load the file into an RDD\n",
    "rdd_file = sc.textFile(file_path)\n",
    "\n",
    "# Verify RDD content\n",
    "print(rdd_file.collect())  # Output: ['Hello, World!', 'This is a sample file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\"> The collect() action retrieves the entire RDD as a list to the driver. This is useful for small datasets or debugging.<span>\n",
    "\n",
    "<span style=\"font-size:16px;\"> We can apply various map and reduce type transformations to these rdd. Examples of some have been given below.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n",
      "[2, 4]\n",
      "['Spark', 'is', 'great', 'RDDs', 'are', 'powerful']\n",
      "15\n",
      "5\n",
      "[1, 2, 3]\n",
      "[('odd', <pyspark.resultiterable.ResultIterable object at 0x12007f9d0>), ('even', <pyspark.resultiterable.ResultIterable object at 0x1200f7bd0>)]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "[1, 2, 3, 4]\n",
      "[('a', 4), ('b', 6)]\n",
      "[('a', 2), ('b', 3), ('c', 4)]\n",
      "[1, 3, 4]\n",
      "[('a', 2), ('b', 3), ('c', 4), ('a', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Apply map transformation to square each element\n",
    "squared_rdd = rdd_list.map(lambda x: x ** 2)\n",
    "\n",
    "# Collect and print the result\n",
    "print(squared_rdd.collect())  # Output: [1, 4, 9, 16, 25]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Apply filter transformation to select even numbers\n",
    "even_rdd = rdd_list.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Collect and print the result\n",
    "print(even_rdd.collect())  # Output: [2, 4]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Apply flatMap transformation to split words in a sentence\n",
    "sentences = [\"Spark is great\", \"RDDs are powerful\"]\n",
    "sentences_rdd = sc.parallelize(sentences)\n",
    "\n",
    "# FlatMap to split sentences into words\n",
    "words_rdd = sentences_rdd.flatMap(lambda sentence: sentence.split(\" \"))\n",
    "\n",
    "# Collect and print the result\n",
    "print(words_rdd.collect())  # Output: ['Spark', 'is', 'great', 'RDDs', 'are', 'powerful']\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Apply reduce to calculate the sum of the elements\n",
    "sum_result = rdd_list.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Print the result\n",
    "print(sum_result)  # Output: 15\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Count the number of elements\n",
    "print(rdd_list.count())  # Output: 5\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Take the first 3 elements\n",
    "print(rdd_list.take(3))  # Output: [1, 2, 3]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Group elements by their parity (even or odd)\n",
    "grouped_rdd = rdd_list.groupBy(lambda x: 'even' if x % 2 == 0 else 'odd')\n",
    "\n",
    "# Collect and print the result\n",
    "print(grouped_rdd.collect())\n",
    "# Output: [('odd', [1, 3, 5]), ('even', [2, 4])] or [('even', iter([2, 4])), ('odd', iter([1, 3, 5]))]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create two RDDs\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([4, 5, 6])\n",
    "\n",
    "# Perform union\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "\n",
    "# Collect and print the result\n",
    "print(union_rdd.collect())  # Output: [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create an RDD with duplicates\n",
    "rdd_with_duplicates = sc.parallelize([1, 2, 2, 3, 3, 4])\n",
    "\n",
    "# Apply distinct transformation\n",
    "distinct_rdd = rdd_with_duplicates.distinct()\n",
    "\n",
    "# Collect and print the result\n",
    "print(distinct_rdd.collect())  # Output: [1, 2, 3, 4]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create an RDD of key-value pairs\n",
    "pair_rdd = sc.parallelize([('a', 1), ('b', 2), ('a', 3), ('b', 4)])\n",
    "\n",
    "# Apply reduceByKey to sum the values with the same key\n",
    "reduced_rdd = pair_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect and print the result\n",
    "print(reduced_rdd.collect())  # Output: [('a', 4), ('b', 6)]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create an RDD of key-value pairs\n",
    "pair_rdd = sc.parallelize([('a', 1), ('b', 2), ('c', 3)])\n",
    "\n",
    "# Apply mapValues to increment each value by 1\n",
    "mapped_values_rdd = pair_rdd.mapValues(lambda x: x + 1)\n",
    "\n",
    "# Collect and print the result\n",
    "print(mapped_values_rdd.collect())  # Output: [('a', 2), ('b', 3), ('c', 4)]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Sample 50% of the RDD elements\n",
    "sampled_rdd = rdd_list.sample(withReplacement=False, fraction=0.5)\n",
    "\n",
    "# Collect and print the result\n",
    "print(sampled_rdd.collect())\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "key_value_rdd = sc.parallelize([('a', 1), ('b', 2), ('c', 3), ('a', 4)])\n",
    "\n",
    "# Apply mapValues to increment each value by 1\n",
    "mapped_values_rdd = key_value_rdd.mapValues(lambda x: x + 1)\n",
    "\n",
    "# Collect and print the result\n",
    "print(mapped_values_rdd.collect())  # Output: [('a', 2), ('b', 3), ('c', 4), ('a', 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\"> One of the flagship features of RDD is lineage tracking. We can see the lineage of a RDD by using the toDebugString() function. We can also observe the break in the lineage by creating a checkpoint using the checkpoint() function.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Lineage: b'(1) PythonRDD[34] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[33] at readRDDFromFile at PythonRDD.scala:289 []'\n",
      "[1, 2, 3]\n",
      "True\n",
      "RDD Lineage after checkpoint: b'(1) PythonRDD[34] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[33] at readRDDFromFile at PythonRDD.scala:289 []\\n |  ReliableCheckpointRDD[35] at collect at /var/folders/58/64zqz_d92v58h90mtvz109r80000gn/T/ipykernel_90709/2310518397.py:7 []'\n"
     ]
    }
   ],
   "source": [
    "rdd_first = sc.parallelize([1, 2, 3])\n",
    "rdd_second = rdd_first.map(lambda x: x * 2)\n",
    "\n",
    "print(\"RDD Lineage:\", rdd_second.toDebugString())\n",
    "\n",
    "rdd_first.checkpoint()\n",
    "print(rdd_first.collect())\n",
    "\n",
    "print(rdd_first.isCheckpointed())\n",
    "\n",
    "print(\"RDD Lineage after checkpoint:\", rdd_second.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\"> To improve performance, spark adopts a lazy execution policy. We can observe this in the following code.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1733190195.251616\n",
      "[1, 4, 9, 16, 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Called with 1 at 1733190195.457449\n",
      "Called with 2 at 1733190195.45758\n",
      "Called with 3 at 1733190195.4575841\n",
      "Called with 4 at 1733190195.4575958\n",
      "Called with 5 at 1733190195.457599\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def square(x):\n",
    "    call_time = time.time()\n",
    "    print(f\"Called with {x} at {call_time}\")\n",
    "    return x ** 2\n",
    "\n",
    "squared_rdd = rdd_list.map(square)\n",
    "exec_time = time.time()\n",
    "print(\"Execution time:\", exec_time)\n",
    "\n",
    "print(squared_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;\"> Another optimization you can use is caching the RDD. A stronger version of this is persist() which can be used to save the Rdd to the disk.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum Without Cache: 15\n",
      "Time Without Cache: 0.12427783012390137\n",
      "Sum With Cache: 15\n",
      "Time With Cache: 0.12604618072509766\n"
     ]
    }
   ],
   "source": [
    "# Without cache\n",
    "start = time.time()\n",
    "print(\"Sum Without Cache:\", rdd_list.reduce(lambda x, y: x + y))\n",
    "print(\"Time Without Cache:\", time.time() - start)\n",
    "\n",
    "# With cache\n",
    "rdd_list.cache()\n",
    "start = time.time()\n",
    "print(\"Sum With Cache:\", rdd_list.reduce(lambda x, y: x + y))\n",
    "print(\"Time With Cache:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:24px;\">Spark UI Overview<span>\n",
    "\n",
    "<span style=\"font-size:16px;\">\n",
    "By default, Spark UI is accessible at http://<driver-host>:4040 while the application is running. If a different address or port is used, pyspark notifies the user during context creation.\n",
    "\n",
    "Key Features:<span>\n",
    "\n",
    "#\n",
    "- <span style=\"font-size:16px;\">Job Tab<span>\n",
    "    - <span style=\"font-size:16px;\">Lists all the jobs in the application with details such as:<span>\n",
    "        - <span style=\"font-size:16px;\">Job ID: A unique identifier for each job.<span>\n",
    "        - <span style=\"font-size:16px;\">Status: Indicates whether a job is completed, running, or failed.<span>\n",
    "        - <span style=\"font-size:16px;\">Duration: The time taken for the job to execute.<span>\n",
    "        - <span style=\"font-size:16px;\">Stages: Shows the stages involved in the job.<span>\n",
    "    - <span style=\"font-size:16px;\">Provides a visualization of the DAG (Directed Acyclic Graph) of stages and their dependencies.<span>\n",
    "\n",
    "- <span style=\"font-size:16px;\">Stages Tab<span>\n",
    "    - <span style=\"font-size:16px;\">Displays detailed information about each stage of the application:<span>\n",
    "        - <span style=\"font-size:16px;\">Stage ID: Unique identifier for each stage.<span>\n",
    "        - <span style=\"font-size:16px;\">Task Summary: Number of tasks, their status (e.g., succeeded, failed, or pending).<span>\n",
    "        - <span style=\"font-size:16px;\">Input/Output Data: Data read/written during the stage.<span>\n",
    "        - <span style=\"font-size:16px;\">Shuffle Read/Write: Details about shuffle operations.<span>\n",
    "    - <span style=\"font-size:16px;\">Aggregated Metrics: Task execution times, shuffle data, and resource usage.<span>\n",
    "\n",
    "- <span style=\"font-size:16px;\">Storage Tab<span>\n",
    "    - <span style=\"font-size:16px;\">Lists all the RDDs or datasets currently cached or persisted.<span>\n",
    "    - <span style=\"font-size:16px;\">Provides details such as:<span>\n",
    "        - <span style=\"font-size:16px;\">RDD ID: Unique identifier for each RDD.<span>\n",
    "        - <span style=\"font-size:16px;\">Storage Level: Type of storage used (e.g., MEMORY_ONLY, DISK_ONLY).<span>\n",
    "        - <span style=\"font-size:16px;\">Partitions: Number of partitions and their sizes.<span>\n",
    "        - <span style=\"font-size:16px;\">Memory/Disk Usage: Amount of data stored in memory or on disk.<span>\n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:24px;\">Analytics using Spark<span>\n",
    "\n",
    "<span style=\"font-size:16px;\"> \n",
    "You are given csv file containing the website logs for the quiz you took. Using these, you have to identify all the cheaters.\n",
    "\n",
    "The csv file has the following columns:\n",
    "1. 'Time': The timestamp in the format \"%d/%m/%y; %H:%M:%S\".\n",
    "2. 'UserId': The unique Id of the user.\n",
    "3. 'Event context': The current screen/webpage visible to the user.\n",
    "4. 'Component': Type of interface visible to the user.\n",
    "5. 'Event name': The action performed by the user.\n",
    "6. 'Description': A detailed description of the action.\n",
    "\n",
    "('Event context' column is a refinement of the 'Component' column.)\n",
    "\n",
    "You can conclude someone is a cheater if they answer a question significantly quicker than the average time for that problem. For generalization, Someone is a cheater if first response time is less than 1/5 times the average for that problem.\n",
    "\n",
    "A suggested roadmap for the problem is:\n",
    "- Load the file into a Rdd.\n",
    "- Convert this Rdd to a Rdd with suitable format like lists or dictionaries.\n",
    "- Remove all logs not related to the quiz.(Hint: Apply a filter on the 'Component' column.)\n",
    "- Group the logs for each user.\n",
    "- For each user, group the logs by the problem type.\n",
    "- For each problem, find when it was first viewed and submitted.\n",
    "- Calculate the time taken for each problem by each user. \n",
    "- Calculate the average time taken for each problem.\n",
    "- Compare these to every user and identify the cheaters.\n",
    "\n",
    "You can also try computing the following:\n",
    "- Which users did not attempt the quiz.\n",
    "- Who was the Early Bird.( Which user was the first to attempt the quiz.)\n",
    "- Who completed all the questions the fastest.\n",
    "- What was the maximum and minimum time taken for each problem.\n",
    "- For each problem, who solved it the quickest.\n",
    "<span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
